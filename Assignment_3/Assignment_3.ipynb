{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modern-wrestling",
   "metadata": {},
   "source": [
    "# Assignment 3 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-finger",
   "metadata": {},
   "source": [
    "### Author :Omer Ozeren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-yield",
   "metadata": {},
   "source": [
    "## Data Description :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-performance",
   "metadata": {},
   "source": [
    "The data that I selected for this assignment is \"adjnoun.gml\".The origin of this data can be found [here](http://networkdata.ics.uci.edu/data/adjnoun).The file adjnoun.gml contains the network of common adjective and noun adjacencies for the novel \"David Copperfield\" by Charles Dickens, as described by M. Newman. Nodes represent the most commonly occurring adjectives and nouns in the book. Node values are 0 for adjectives and 1 for nouns. Edges connect any pair of words that occur in adjacent position in the text of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tested-daughter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 112\n",
      "Number of edges: 425\n",
      "Average degree:   7.5893\n",
      "Diameter:5\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "# upload file\n",
    "file = 'adjnoun.gml'\n",
    "G = nx.read_gml(file)\n",
    "# information\n",
    "print(nx.info(G))\n",
    "# largest number of vertices to be travelled between one certex to another.\n",
    "print('Diameter:{}'.format(nx.diameter(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-basketball",
   "metadata": {},
   "source": [
    "## Data Loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-telephone",
   "metadata": {},
   "source": [
    "The data will be read into the Jupyter notebook environment directly from the GitHub url where the data is being kept.I will use networkx to upload the data into notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-gentleman",
   "metadata": {},
   "source": [
    "##  Hypothetical Outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-amplifier",
   "metadata": {},
   "source": [
    "The degree centrality is a measure of the number of neighbors a node has. The concept of \"Centrality\" gives an idea of the measure of the number of words connected to it. It's estimated by number of neighbors connected to a node, divided by the total number of nodes. In my case, I'll compute the number of neighbors connected to each word. From there, I can use estimated degree centrality to predict the words of a particular blog belong to adjectives or nouns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
